---
title: "R Notebook"
output: html_notebook
---

```{r setup}
knitr::opts_chunk$set(
  cache = TRUE
)

library(arrow) # data retrieval from storage
library(quanteda) # general purpose text analysis
library(spacyr) # lemmatisation, pos tagging
library(stm) # topic models
library(tidyverse) # dataframe manipulation
library(wordcloud) # wordcloud visualization

spacy_initialize(model = "en_core_web_trf")
```

```{r}
# Function to read in data from storage 
read_reddit_data <- function(subreddit) {
  path = paste0('data/', subreddit)
  df <- read_feather(path)
  return(df)
}
```


```{r}
cmv <- read_reddit_data('changemyview')
head(cmv, 2)
```

```{r}
# Pre-processing before tokenizing
# Combine the title and post text
# Remove URLS and zero-width spaces &#x200B
url_pattern <- "[(]?http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"
zero_width_space_pattern <- "&#x200(b|B)"

cmv <- cmv %>%
  mutate(text_with_title = paste(title, selftext)) %>%
  select(!c("selftext")) %>%
  mutate(text_with_title = str_replace_all(text_with_title, url_pattern, ''),
         text_with_title = str_replace_all(text_with_title, zero_width_space_pattern, ''))
```

```{r}
head(cmv, 2)
```

```{r}
#rd_posts <- rd[!duplicated(rd$post_text),] %>%
#  mutate(complete_text = str_replace_all(paste(title, post_text), "\031", "'"))
```

```{r}
# Create a corpus from the text of posts 
(cmv_text_corpus <- corpus(cmv, text_field = 'text_with_title'))
```




```{r}
(cmv_text_summary <- textstat_summary(corpus(cmv_url, text_field = 'full_text')))
```

```{r}
# TODO: Differentiate adverbs of manner
# Tokenize corpus
# pre-processing steps after tokenizing
# lemmatization, removal of noisy POS tags (e.g. PUNCT), stopword removal, ngram detection, removal of urls
cmv_spacy_tokens <- spacy_parse(cmv_text_corpus) %>% 
        subset(!pos %in% c('ADP', 'ADV', 'AUX', 'PART', 'PUNCT', 'SPACE', 'SYM')) %>%
        subset(!entity %in% c('PERCENT_I'))
        #subset(!pos %in% c('VERB', , 'X', 'NUM' 'ADV', 'PART',)) %>% 
  
# Convert from Spacy to Quanteda
cmv_text_tokens <- cmv_spacy_tokens %>%
        as.tokens(use_lemma = TRUE) %>%
        #tokens_remove(pattern = c("amp;"), valuetype = "regex", padding = TRUE) %>% 
        tokens_remove(pattern = c("_"), valuetype = "fixed", padding = TRUE) %>% 
        tokens_remove(stopwords("en"), padding = TRUE) %>%
        tokens_ngrams(n = 1:2) 
        #tokens_remove(pattern = c("?"), padding = TRUE) # Remove single character tokens

# Create document feature matrix
# Trim dfm to remove features that appear less than 5 times in the corpus 
# and those that appear in more than 90% of the documents
(cmv_dfm <- cmv_text_tokens %>%
        dfm() %>%
        dfm_trim(min_termfreq = 5, max_docfreq = 0.7, docfreq_type = 'prop')
)

# Spacy removes the docvars - re-attach docvars from the initial corpus
docvars(cmv_dfm) <- docvars(cmv_text_corpus)

# Convert dfm into a format suitable for STM input
stm_input <- convert(cmv_dfm, to = "stm")
```



```{r}
# Choose a number of appropriate topics
k_search_output <- searchK(stm_input$documents, 
                         stm_input$vocab,
                         K = c(3:50), 
                         data = stm_input$meta,
                         heldout.seed = 123, 
                         verbose = FALSE)

plot(k_search_output)
```


```{r}
k_search_output
```

```{r}
# Plot exclusion
plot(k_search_output$K, )
```



```{r, fig.height = 5, fig.width = 5}
# Create model with optimum number of topics
cmv_model <- stm(stm_input$documents, stm_input$vocab, K = 20,
               data = stm_input$meta, verbose = FALSE,
               init.type = c("Spectral"))

plot(cmv_model)
```



```{r}
# Wordclouds
c1 <- cloud(cmv_model7, topic = 2, scale = c(2,.25))
c2 <- cloud(cmv_model7, topic = 2, scale = c(2,.25))
c3 <- cloud(cmv_model7, topic = 3, scale = c(2,.25))
c4 <- cloud(cmv_model7, topic = 4, scale = c(2,.25))
c5 <- cloud(cmv_model7, topic = 5, scale = c(2,.25))
c6 <- cloud(cmv_model7, topic = 6, scale = c(2,.25))
c7 <- cloud(cmv_model7, topic = 7, scale = c(2,.25))
c8 <- cloud(cmv_model7, topic = 8, scale = c(2,.25))
c9 <- cloud(cmv_model7, topic = 9, scale = c(2,.25))
c10 <- cloud(cmv_model7, topic = 10, scale = c(2,.25))
```
```{r}
# Get topic proportions for each document
doc_topic_proportions <- make.dt(cmv_model, meta = stm_input$meta)
# Add a new variable referring to the most likely topic for the document
doc_topic_proportions$max_topic = max.col(doc_topic_proportions[,Topic1:Topic5])
```

```{r}
doc_topic_proportions
```


```{r}
doc_topic_proportions[, c('title', 'max_topic')] %>%
  arrange(max_topic)
```





