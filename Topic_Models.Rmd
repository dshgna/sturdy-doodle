---
title: "R Notebook"
output: html_notebook
---

```{r setup}
knitr::opts_chunk$set(
  cache = TRUE
)

library(arrow) # data retrieval from storage
library(memoise) # caching
library(quanteda) # general purpose text analysis
library(quanteda.textstats) # descriptive statistics for text
library(spacyr) # lemmatisation, pos tagging
library(stm) # topic models
library(tictoc) # timer
library(tidyverse) # dataframe manipulation
library(wordcloud) # wordcloud visualization
library(cld2) # remove non-English words

source('utility_functions.R')

set.seed(123)
```

```{r}
main_df <- read_reddit_data('combined_preprocessed')
head(main_df, 2)
```

```{r}
dim(main_df)
```


```{r}
# Pre-processing before tokenizing

# Remove URLS and zero-width spaces &#x200B
url_pattern <- "[(\\[]?http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"
replace_characters <- "(&amp;|\n|\r|#x200b;|&gt;)"
replace_word_list <- "()"
replace_title_word_list <- "cmv:"

main_df_pp <- main_df %>%
    mutate(
        selftext = tolower(selftext),
        title = tolower(title),
        selftext = str_replace_all(selftext, url_pattern, ''), 
        selftext = str_replace_all(selftext, replace_characters, ''),
        title = str_replace_all(title, replace_title_word_list, '')
    ) %>%
    filter(selftext != '') %>% 
    mutate(text_with_title = paste(title, selftext)) %>% 
    distinct(text_with_title, .keep_all = TRUE) # Remove duplicated text with title (main_df_pp[!duplicated(main_df_pp$text_with_title),])

#%>% select(!c("selftext")) 

dim(main_df_pp)
```

```{r create_corpus}
# Create corpus
(main_text_corpus <- corpus(main_df_pp, text_field = 'text_with_title'))
```


```{r text_stat_summary}
# Quanteda Test stat summary
main_text_summary <- textstat_summary(main_text_corpus)
(main_text_summary <- cbind(subreddit = main_df_pp$subreddit, main_text_summary))
```

```{r}
# View samples from each subreddit to identify what to preprocess
subreddits <- unique(main_df_pp$subreddit)

tic()

map(subreddits, function(s) { ## only need to swap `lapply` for `map`
    subreddit_df <- main_df_pp %>%
        filter(subreddit == s) %>%
        select(text_with_title)
   return(head(subreddit_df))
})

toc()
# for loops: 0.561
# purr: 0.767
```

```{r}
# Updated Descriptive statistics after pre-processing

# Number of Posts
p1_pp <- ggplot(main_df_pp, aes(x = subreddit, fill = subreddit)) +
        geom_bar(alpha = 0.6) +
        labs(title = "Number of Posts", x = "", y = "") +
        theme_minimal() +
        theme(legend.position = "none", 
              plot.title = element_text(size=11), 
              axis.text.x = element_blank()) +
        scale_fill_manual(values = color_palette) +
        scale_x_discrete(labels = subreddit_names) +
        scale_y_continuous(trans = 'sqrt', labels = comma)

# Tokens per Post
p2_pp <- main_text_summary %>%
  ggplot( aes(x = subreddit, y = tokens, fill = subreddit)) +
    geom_violin(alpha = 0.6) +
    theme_minimal() +
    theme(
      legend.position = "none",
      plot.title = element_text(size = 11),
            axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
    labs(title = "Words per Post", x = "", y = "") +
    scale_fill_manual(values = color_palette) +
    scale_x_discrete(labels = subreddit_names) +
    scale_y_continuous(trans = 'sqrt')
  #coord_trans(y = trans_cube)
```

```{r fig.height = 10}
grid.arrange(p1_pp, p2_pp, ncol = 1)
```

```{r}
subreddits <- unique(main_df_pp$subreddit)
```

```{r}
# Function to parse one batch of tokens
batch_token_parse <- function(s) {
    print(s)
    # Subset corpus
    if(s == 'unpopularopinion') {
        # Given the large size of unpopularopinion, randomly sample 70000 documents
        s_corpus <- corpus_subset(main_text_corpus, subreddit == s)
        s_corpus <- corpus_sample(s_corpus, size = 70000)
    } else {
        s_corpus <- corpus_subset(main_text_corpus, subreddit == s)
    }
  
    # Tokenise and remove noisy POS tags (e.g. PUNCT)
    spacy_tokens <- spacy_parse(s_corpus) %>% 
        subset(!pos %in% c('ADP', 'ADV', 'AUX', 'PART', 'PUNCT', 'SPACE', 'SYM', 'NUM')) %>% # lemmatization, removal of n
        subset(!entity %in% c('PERCENT_I'))
   return(spacy_tokens)
  
  # TODO Maybe: subset(!pos %in% c('VERB', , 'X', 'NUM' 'ADV', 'PART',)) %>%
  # TODO Wishlist: Differentiate adverbs of manner
}

# Memoise for caching for each function call
batch_token_parse <- memoise(batch_token_parse)

tic()
# Batch parse all tokens
all_tokens <- map(subreddits, batch_token_parse)
toc()
```


```{r save_tokens}
# Save extracted tokens
saveRDS(all_tokens, file = "../data/all_tokens.rds")
```


```{r}
all_tokens
```
```{r read_tokens}
all_tokens <- readRDS(file = "../data/all_tokens.rds")
```


```{r}
# Combine token outputs from batch processing together
spacy_tokens <- bind_rows(all_tokens)
```

```{r}
spacy_tokens
```



```{r}
# Convert from Spacy to Quanteda
(cc_text_tokens <- spacy_tokens %>%
        as.tokens(use_lemma = TRUE) %>% # Lemmatisation
        tokens_remove(stopwords("en"), padding = TRUE) %>% # stopword removal
        tokens_keep(min_nchar = 2) # Remove single character tokens)
 )
```


```{r save_parsed_tokens}
# Save parsed tokens
saveRDS(cc_text_tokens, file = "../data/parsed_tokens.rds")
```

```{r read_parsed_tokens}
cc_text_tokens <- readRDS(file = "../data/parsed_tokens.rds")
```


```{r collocations}
# Identify collocations
(talk_collocations <- cc_text_tokens %>% 
                  textstat_collocations(min_count = 1000, size = 2:3))
```


```{r}
# Create document feature matrix
# Compound collocations
# Trim dfm to remove features that appear less than 5 times in the corpus 
# and those that appear in more than 70% of the documents

(cc_dfm <- cc_text_tokens %>%
        tokens_compound(pattern = talk_collocations[talk_collocations$z > 150]) %>%
        dfm() %>%
        dfm_trim(min_termfreq = 5, max_docfreq = 0.7, docfreq_type = 'prop')
)   
```


```{r}
# Find out dropped documents when pre-processing and converted to dfm
cp <- docnames(main_text_corpus)
d <- docnames(cc_dfm)
retained <- cp[(cp %in% d)]
```

```{r}
# Spacy removes the docvars - re-attach docvars from the initial corpus
docvars(main_text_corpus, 'docid') <- docid(main_text_corpus)
retained_corpus <- corpus_subset(main_text_corpus, docid %in% retained)
docvars(cc_dfm) <- docvars(retained_corpus)
```

```{r save_dfm}
# Save STM input
saveRDS(cc_dfm, file = "../data/cc_dfm.rds")
```


```{r}
sample_dfm = dfm_sample(cc_dfm, size = 80000)
```

```{r}
stm_sample_input <- convert(sample_dfm, to = "stm")
```


```{r}
# Convert dfm into a format suitable for STM input
stm_input <- convert(cc_dfm, to = "stm")
```


```{r save_stm_input}
# Save STM input
saveRDS(stm_input, file = "../data/stm_input.rds")
```

```{r read_saved_input}
stm_input <- readRDS(file = "../data/stm_input.rds")
cc_dfm <- readRDS(file = "../data/cc_dfm.rds")
```

```{r}
dim(stm_input)
```



```{r}
# Choose a number of appropriate topics
tic()
k_search_output <- searchK(stm_input$documents, 
                        stm_input$vocab,
                        K = c(10, 20, 25, 30, 35, 40, 50), 
                        prevalence = ~ subreddit,
                        #content = ~ subreddit, 
                        data = stm_input$meta,
                        heldout.seed = 123, 
                        verbose = TRUE,
                        cores = 8)
toc()
```


```{r}
plot(k_search_output)
```


```{r}
values <- data.frame(K = unlist(k_search_output$results$K),
                     exlusivity = unlist(k_search_output$results$exclus),
                    semcoh = unlist(k_search_output$results$semcoh),
 heldout = unlist(k_search_output$results$heldout))
 
```

```{r fig.height = 3, fig.width = 10}
eval_1 <- ggplot(values, aes(K, heldout)) +
    geom_point(color = "#9e6ebd") +
    geom_line(color = "#9e6ebd") +
  labs(title = "Heldout Likelihood", x = "K", y = "") +
    theme_minimal()
eval_2 <- ggplot(values, aes(K, semcoh)) +
    geom_point(color = "#7aa457") +
    geom_line(color = "#7aa457") +
  labs(title = "Semantic Coherence", x = "K", y = "") +
    theme_minimal()
eval_3 <- ggplot(values, aes(K, exlusivity)) +
    geom_point(color = "#cb6751") +
  geom_line(color = "#cb6751") +
  labs(title = "Exclusivity", x = "K", y = "") +
    theme_minimal()

grid.arrange(eval_1, eval_2, eval_3, nrow = 1)
```



```{r, fig.height = 5, fig.width = 5}
# Create model with optimum number of topics
cmv_model <- stm(stm_input$documents, stm_input$vocab, K = 20,
               data = stm_input$meta, verbose = TRUE,
               init.type = c("Spectral"))

plot(cmv_model)
```


```{r}
saveRDS(cmv_model, file = "../data/cmv_model.rds")
```


```{r}
# Explore tokens in a document
#kwic(cmv_spacy_tokens %>% as.tokens(use_lemma = TRUE), pattern = "'", valuetype = "regex", window = 10)
```


```{r}
# Wordclouds
c1 <- cloud(cmv_model, topic = 1, scale = c(3,.25), color = "#89ecff")
c2 <- cloud(cmv_model, topic = 2, scale = c(3,.25), color = "#cf2700")
c3 <- cloud(cmv_model, topic = 3, scale = c(3,.25), color = "#2cffea")
c4 <- cloud(cmv_model, topic = 4, scale = c(3,.25), color = "#de00a7")
c5 <- cloud(cmv_model, topic = 5, scale = c(3,.25), color = "#00dd5f")
c6 <- cloud(cmv_model, topic = 6, scale = c(3,.25), color = "#6f0089")
c7 <- cloud(cmv_model, topic = 7, scale = c(3,.25), color = "#abff5c")
c8 <- cloud(cmv_model, topic = 8, scale = c(3,.25), color = "#6a85ff")
c9 <- cloud(cmv_model, topic = 9, scale = c(3,.25), color = "#c5b800")
c10 <- cloud(cmv_model, topic = 10, scale = c(3,.25), color = "#000d2e")
c11 <- cloud(cmv_model, topic = 11, scale = c(3,.25), color = "#02ea9c")
c12 <- cloud(cmv_model, topic = 12, scale = c(3,.25), color = "#d00052")
c13 <- cloud(cmv_model, topic = 13, scale = c(3,.25), color = "#02b9de")
c14 <- cloud(cmv_model, topic = 14, scale = c(3,.25), color = "#770a00")
c15 <- cloud(cmv_model, topic = 15, scale = c(3,.25), color = "#00426c")
c16 <- cloud(cmv_model, topic = 16, scale = c(3,.25), color = "#a9bbff")
c17 <- cloud(cmv_model, topic = 17, scale = c(3,.25), color = "#8e6d00")
c18 <- cloud(cmv_model, topic = 18, scale = c(3,.25), color = "#006d53")
c19 <- cloud(cmv_model, topic = 19, scale = c(3,.25), color = "#ff7473")
c20 <- cloud(cmv_model, topic = 20, scale = c(3,.25), color = "#683700")
```


```{r}
docs_with_text <- docvars(cc_dfm) %>%
    mutate(text_with_title = paste(title, selftext))
```




```{r}
for (x in 1:20) {
  print('NEW TOPIC')
  print(x)
	sample_tweet <- findThoughts(cmv_model,
	texts = docs_with_text$text_with_title[rowSums(cc_dfm) > 0],
	n = 20, topics = x)$docs[[1]]
	print(sample_tweet)
	#plotQuote(sample_tweet, width = 50,
	#       main = paste("Documents containing topic ", x))
}
```


```{r}
effect_estimates <- estimateEffect(c(1:20) ~ subreddit, cmv_model, meta = stm_input$meta)
```


```{r topic_proportions}
# Get topic proportions for each document
doc_topic_proportions <- make.dt(cmv_model, meta = stm_input$meta)
# Add a new variable referring to the most likely topic for the document
doc_topic_proportions$max_topic = max.col(doc_topic_proportions[,Topic1:Topic20])
head(doc_topic_proportions)
```




```{r}
subreddit_topic_prop <- doc_topic_proportions %>%
  group_by(subreddit, max_topic) %>%
  summarise(value = n()) %>%
  mutate(percent = value/sum(value)*100)
```

```{r fig.height = 10}
ggplot(subreddit_topic_prop, aes(subreddit, max_topic, fill = percent)) + 
  geom_tile() +
  labs(title = "Topic Distribution across Polarised Subreddits", x = "", fill = "% Topic", y ="") +
  theme_minimal() +
  theme(plot.title = element_text(size = 11),
      axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  scale_fill_manual(drop=FALSE, values=colorRampPalette(c("white","red"))(5), na.value="#EEEEEE", name="Times") 
  #scale_fill_gradient(low = "#dfecd1", high = "#863568") 
#+
#  scale_x_discrete(labels = subreddit_names)
```






```{r}
doc_topic_proportions[, c('title', 'max_topic')] %>%
  arrange(max_topic)
```





