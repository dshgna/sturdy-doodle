---
title: "R Notebook"
output: html_notebook
---

```{r setup}
library(RedditExtractoR)
library(quanteda)
library(spacyr)
library(stm)
library(tidyverse)
library(wordcloud)

spacy_initialize(model = "en_core_web_sm")
```

```{r}
# collecting up to 5 pages of posts in r/changemyview
rd <- get_reddit(subreddit = "changemyview", sort = "new", page_threshold = 5)
```


```{r}
head(rd)
```

```{r}
# deleting bot messages and removed messages
rd <- rd[rd$user != "DeltaBot", ]
```

```{r}
head(rd)
```
```{r}
# finding comments that gave deltas
rd$delta <- grepl("!delta|#8710|\006", rd$comment)
```
```{r}
# Removing extraneous columns
rd <- rd %>%
  select(!c('subreddit', 'domain'))
```


```{r}
head(rd[rd$delta == TRUE,])
```

```{r}
rd[55:63,]
```



```{r}
# finding comments that changed people's minds
deltas <- which(rd$delta) # comments that assign deltas
```

```{r}
rd_posts <- rd[!duplicated(rd$post_text),] %>%
  mutate(complete_text = str_replace_all(paste(title, post_text), "\031", "'"))
```

```{r}
# Create a corpus from the text of posts 
(cmv_text_corpus <- corpus(rd_posts, text_field = 'complete_text'))
```


```{r}
test <- spacy_parse(cmv_text_corpus, pos = TRUE, tag = TRUE, lemma = TRUE) %>% 
        subset(!pos %in% c('VERB', 'SYM', 'PUNCT', 'X', 'NUM', 'SPACE', 'ADV', 'PART', 'AUX'))

test %>%
  subset(token = '*')
```



```{r}
# TODO: Add URL removal
# spacyR used to lemmatise instead of wordstems
cmv_text_tokens <- spacy_parse(cmv_text_corpus, pos = TRUE, tag = TRUE, lemma = TRUE) %>% 
        subset(!pos %in% c('VERB', 'SYM', 'PUNCT', 'X', 'NUM', 'SPACE', 'ADV', 'PART', 'AUX')) %>% 
        as.tokens(use_lemma = TRUE) %>%
        tokens_remove(pattern = c("amp;"), valuetype = "regex", padding = TRUE) %>% 
        tokens_remove(pattern = c("?"), padding = TRUE) %>% # Remove single character tokens
  #tokens_remove(pattern = c("_", "%", "*", "-"), valuetype = "fixed", padding = TRUE) %>% 
        tokens_remove(stopwords("en"), padding = TRUE) 
#%>%
       # tokens_ngrams(n = 1:2) 
   
(cmv_text_dfm <- cmv_text_tokens %>%
        dfm() %>%
        dfm_trim(min_termfreq = 5, max_docfreq = 0.5, docfreq_type = 'prop')
)

docvars(cmv_text_dfm) <- docvars(cmv_text_corpus)
```


```{r}
# remove ï¿½
kwic(cmv_text_tokens, pattern = "-", valuetype = "fixed", window = 10)
```



```{r}
featfreq(cmv_text_dfm)
```


```{r}
# Convert dfm into a format suitable for STM input
stm_input <- convert(cmv_text_dfm, to = "stm")
```

```{r}
# Choose a number of appropriate topics
k_search_output <- searchK(stm_input$documents, 
                         stm_input$vocab,
                         K = c(3:50), 
                         data = stm_input$meta,
                         heldout.seed = 123, 
                         verbose = FALSE)

plot(k_search_output)
```





```{r}
# Suggested best model
#scale(k_search_output$results$K, center = TRUE, scale = TRUE)
```

```{r}
# Create model with optimum number of topics
cmv_model <- stm(stm_input$documents, stm_input$vocab, K = 30,
               data = stm_input$meta, verbose = FALSE,
               init.type = c("Spectral"))
```


```{r}
plot(cmv_model)
```
```{r}
# Wordclouds
c1 <- cloud(cmv_model, topic = 1, scale = c(2,.25))
c2 <- cloud(cmv_model, topic = 2, scale = c(2,.25))
c3 <- cloud(cmv_model, topic = 3, scale = c(2,.25))
c4 <- cloud(cmv_model, topic = 4, scale = c(2,.25))
c5 <- cloud(cmv_model, topic = 5, scale = c(2,.25))
#c6 <- cloud(cmv_model, topic = 6, scale = c(2,.25))
#c7 <- cloud(cmv_model, topic = 7, scale = c(2,.25))
#c8 <- cloud(cmv_model, topic = 8, scale = c(2,.25))
#c9 <- cloud(cmv_model, topic = 9, scale = c(2,.25))
#c10 <- cloud(cmv_model, topic = 10, scale = c(2,.25))
```
```{r}
# Get topic proportions for each document
doc_topic_proportions <- make.dt(cmv_model, meta = stm_input$meta)
# Add a new variable referring to the most likely topic for the document
doc_topic_proportions$max_topic = max.col(doc_topic_proportions[,Topic1:Topic5])
```

```{r}
doc_topic_proportions
```


```{r}
doc_topic_proportions[, c('title', 'max_topic')] %>%
  arrange(max_topic)
```





