---
title: "R Notebook"
output: html_notebook
---

```{r setup}
knitr::opts_chunk$set(
  cache = TRUE
)

library(arrow) # data retrieval from storage
library(memoise) # caching
library(quanteda) # general purpose text analysis
library(quanteda.textstats) # descriptive statistics for text
library(spacyr) # lemmatisation, pos tagging
library(stm) # topic models
library(tictoc) # timer
library(tidyverse) # dataframe manipulation
library(wordcloud) # wordcloud visualization

source('utility_functions.R')
```

```{r}
main_df <- read_reddit_data('combined_preprocessed')
head(main_df, 2)
```

```{r}
dim(main_df)
```


```{r}
# Pre-processing before tokenizing

# Remove URLS and zero-width spaces &#x200B
url_pattern <- "[(\\[]?http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"
replace_characters <- "(&amp;|\n|\r|#x200b;|&gt;)"
replace_title_word_list <- "cmv:"

main_df_pp <- main_df %>%
    mutate(
        selftext = tolower(selftext),
        title = tolower(title),
        selftext = str_replace_all(selftext, url_pattern, ''), 
        selftext = str_replace_all(selftext, replace_characters, ''),
        title = str_replace_all(title, replace_title_word_list, '')
    ) %>%
    filter(selftext != '') %>% 
    mutate(text_with_title = paste(title, selftext)) %>% 
    distinct(text_with_title, .keep_all = TRUE) # Remove duplicated text with title (main_df_pp[!duplicated(main_df_pp$text_with_title),])

#%>% select(!c("selftext")) 

dim(main_df_pp)
```

```{r create_corpus}
# Create corpus
(main_text_corpus <- corpus(main_df_pp, text_field = 'text_with_title'))
```


```{r text_stat_summary}
# Quanteda Test stat summary
main_text_summary <- textstat_summary(main_text_corpus)
(main_text_summary <- cbind(subreddit = main_df_pp$subreddit, main_text_summary))
```

```{r}
# View samples from each subreddit to identify what to preprocess
subreddits <- unique(main_df_pp$subreddit)

tic()

map(subreddits, function(subreddit) { ## only need to swap `lapply` for `map`
    subreddit_df <- main_df_pp %>%
        filter(subreddit == subreddit) %>%
        select(text_with_title)
   return(head(subreddit_df))
})

toc()
# for loops: 0.561
# purr: 0.767
```

```{r}
# Updated Descriptive statistics after pre-processing

# Number of Posts
p1_pp <- ggplot(main_df_pp, aes(x = subreddit, fill = subreddit)) +
        geom_bar(alpha = 0.6) +
        labs(title = "Number of Posts", x = "", y = "") +library(quanteda)
        theme_minimal() +
        theme(legend.position = "none", 
              plot.title = element_text(size=11), 
              axis.text.x = element_blank()) +
        scale_fill_manual(values = color_palette) +
        scale_x_discrete(labels = subreddit_names) +
        scale_y_continuous(trans = 'sqrt', labels = comma)

# Tokens per Post
p2_pp <- main_text_summary %>%
  ggplot( aes(x = subreddit, y = tokens, fill = subreddit)) +
    geom_violin(alpha = 0.6) +
    theme_minimal() +
    theme(
      legend.position = "none",
      plot.title = element_text(size = 11),
            axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
    labs(title = "Words per Post", x = "", y = "") +
    scale_x_discrete(labels = subreddit_names) +
    scale_y_continuous(trans = 'sqrt')
  #coord_trans(y = trans_cube)
```

```{r fig.height = 10}
grid.arrange(p1_pp, p2_pp, ncol = 1)
```



```{r}
# Function to parse one batch of tokens
batch_token_parse <- function(s) {
    print(s)
    # Subset corpus
    s_corpus <- corpus_subset(main_text_corpus, subreddit == s)
    cmv_spacy_tokens <- spacy_parse(s_corpus) %>% 
        subset(!pos %in% c('ADP', 'ADV', 'AUX', 'PART', 'PUNCT', 'SPACE', 'SYM', 'NUM')) %>%
        subset(!entity %in% c('PERCENT_I'))
   return(cmv_spacy_tokens)
}

# Memoise for caching
batch_token_parse <- memoise(batch_token_parse)

tic()
# Batch parse all tokens
all_tokens <- map(subreddits[1:2], batch_token_parse)
toc()
```





```{r}
# TODO: Differentiate adverbs of manner
# Tokenize corpus
# pre-processing steps after tokenizing
# lemmatization, removal of noisy POS tags (e.g. PUNCT), stopword removal, ngram detection, removal of urls
cmv_spacy_tokens <- spacy_parse(main_text_corpus) %>% 
        subset(!pos %in% c('ADP', 'ADV', 'AUX', 'PART', 'PUNCT', 'SPACE', 'SYM', 'NUM')) %>%
        subset(!entity %in% c('PERCENT_I'))
        #subset(!pos %in% c('VERB', , 'X', 'NUM' 'ADV', 'PART',)) %>%
```


```{r}
# Convert from Spacy to Quanteda
(cmv_text_tokens <- cmv_spacy_tokens %>%
        as.tokens(use_lemma = TRUE) %>%
        #tokens_remove(pattern = c("amp;"), valuetype = "regex", padding = TRUE) %>% 
        tokens_remove(pattern = c("_"), valuetype = "fixed", padding = TRUE) %>% 
        tokens_remove(stopwords("en"), padding = TRUE) %>%
        tokens_ngrams(n = 1:2) 
        #tokens_remove(pattern = c("?"), padding = TRUE) # Remove single character tokens)
 )
```


```{r}
# Create document feature matrix
# Trim dfm to remove features that appear less than 5 times in the corpus 
# and those that appear in more than 90% of the documents
(cmv_dfm <- cmv_text_tokens %>%
        dfm() %>%
        dfm_trim(min_termfreq = 5, max_docfreq = 0.7, docfreq_type = 'prop')
)

# Spacy removes the docvars - re-attach docvars from the initial corpus
docvars(cmv_dfm) <- docvars(cmv_text_corpus)

# Convert dfm into a format suitable for STM input
stm_input <- convert(cmv_dfm, to = "stm")
```

```{r}
# Choose a number of appropriate topics
tic()
k_search_output <- searchK(stm_input$documents, 
                        stm_input$vocab,
                        K = c(10:20), 
                        prevalence = ~ subreddit,
                        content = ~ subreddit, 
                        data = stm_input$meta,
                        heldout.seed = 123, 
                        verbose = TRUE,
                        cores = 8)
toc()
plot(k_search_output)
```


```{r}
k_search_output
```

```{r}
# TODO: Plot exclusion measure
plot(k_search_output$K, )
```



```{r, fig.height = 5, fig.width = 5}
# Create model with optimum number of topics
cmv_model <- stm(stm_input$documents, stm_input$vocab, K = 20,
               data = stm_input$meta, verbose = FALSE,
               init.type = c("Spectral"))

plot(cmv_model)
```

```{r}
# Explore tokens in a document
kwic(cmv_spacy_tokens %>% as.tokens(use_lemma = TRUE), pattern = "'", valuetype = "regex", window = 10)
```


```{r}
# Wordclouds
c1 <- cloud(cmv_model, topic = 2, scale = c(2,.25))
c2 <- cloud(cmv_model, topic = 2, scale = c(2,.25))
c3 <- cloud(cmv_model, topic = 3, scale = c(2,.25))
c4 <- cloud(cmv_model, topic = 4, scale = c(2,.25))
c5 <- cloud(cmv_model, topic = 5, scale = c(2,.25))
c6 <- cloud(cmv_model, topic = 6, scale = c(2,.25))
c7 <- cloud(cmv_model, topic = 7, scale = c(2,.25))
c8 <- cloud(cmv_model, topic = 8, scale = c(2,.25))
c9 <- cloud(cmv_model, topic = 9, scale = c(2,.25))
c10 <- cloud(cmv_model, topic = 10, scale = c(2,.25))
```
```{r}
# Get topic proportions for each document
doc_topic_proportions <- make.dt(cmv_model, meta = stm_input$meta)
# Add a new variable referring to the most likely topic for the document
doc_topic_proportions$max_topic = max.col(doc_topic_proportions[,Topic1:Topic5])
```

```{r}
doc_topic_proportions
```


```{r}
doc_topic_proportions[, c('title', 'max_topic')] %>%
  arrange(max_topic)
```





